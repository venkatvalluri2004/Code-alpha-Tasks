# -*- coding: utf-8 -*-
"""2handwritten character recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z_x3KQV6GKyu9ys1YVoqmhg2GboyAOGG
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import cv2
import joblib
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import MinMaxScaler

# Load Digits Dataset
digits_data = datasets.load_digits()

# Display Sample Images
plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(digits_data.images[i], cmap='gray', interpolation='nearest')
    plt.axis('off')
plt.suptitle("Sample Handwritten Digits", fontsize=14)
plt.show()

# Prepare Data
X = digits_data.data
y = digits_data.target

# Normalize Data (Scale pixel values between 0 and 1)
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Split into Training and Testing Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize Data (Scale pixel values between 0 and 1)
X_train = X_train / 255.0
X_test = X_test / 255.0

X_train

X_test

# Define Models
models = {
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "AdaBoost": AdaBoostClassifier(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42),
    "Support Vector Machine": SVC(kernel='linear', random_state=42),
    "MLP Neural Network": MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)
}

Kmoodel = KNeighborsClassifier(n_neighbors=5)
Kmoodel.fit(X_train, y_train)

kpredictions = Kmoodel.predict(X_test)

score = accuracy_score(y_test, kpredictions)*100
print(f"Accuracy: {score:.4f}%")

Rmodel = RandomForestClassifier(n_estimators=100, random_state=42)
Rmodel.fit(X_train, y_train)

Rpredict = Rmodel.predict(X_test)

Rscore = accuracy_score(y_test, Rpredict)*100
print(f"Rscore: {score:.4f}%")

# Train and Evaluate Models
results = {}
conf_matrices = {}
trained_models = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions) * 100
    results[name] = accuracy
    conf_matrices[name] = confusion_matrix(y_test, predictions)

# Store confusion matrix
conf_matrices[name] = confusion_matrix(y_test, predictions)

# Save trained model
joblib.dump(model, f"{name.replace(' ', '_')}_model.pkl")
trained_models[name] = model

# Convert results into a DataFrame and Display
results_df = pd.DataFrame(list(results.items()), columns=["Model", "Accuracy (%)"])
print("\nModel Performance Comparison:")
print(results_df)

# Plot Performance of Different Models
plt.figure(figsize=(8, 5))
plt.bar(results.keys(), results.values(), color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])
plt.xlabel("Models")
plt.ylabel("Accuracy (%)")
plt.title("Comparison of Model Performance")
plt.xticks(rotation=15)
plt.ylim(90, 100)
plt.show()

# Display Confusion Matrices for Each Model
for name, matrix in conf_matrices.items():
    plt.figure(figsize=(6, 4))
    sns.heatmap(matrix, annot=True, fmt="d", cmap="Blues", xticklabels=digits_data.target_names, yticklabels=digits_data.target_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title(f"Confusion Matrix: {name}")
    plt.show()

# Test a Random Sample from Test Set
random_index = np.random.randint(0, len(X_test))
test_sample = X_test[random_index].reshape(1, -1)
best_model = trained_models["MLP Neural Network"]
predicted_label = best_model.predict(test_sample)

# Predict using the best model (choosing KNN for now)
best_model = models["K-Nearest Neighbors"]
predicted_label = best_model.predict(test_sample)

# Display the Test Image with Prediction
plt.imshow(X_test[random_index].reshape(8, 8), cmap='gray')
plt.title(f"Predicted: {predicted_label[0]}")
plt.axis('off')
plt.show()